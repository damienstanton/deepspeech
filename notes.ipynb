{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access local .py files in the deepspeech dir\n",
    "%rehashx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an acoustic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw audio can be described as a 1D vector:\n",
    "\n",
    "$X = [x_1, x_2 ...]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing\n",
    "+ Two ways to start:\n",
    "  + Minimally pre-process, such as using as simple spectorgram\n",
    "  + This is going away with autoencoding-style deep networks that can go directly from an audio source to a label\n",
    "  \n",
    "Spectrogram idea:\n",
    "+ take a small window, say 20ms of waveform\n",
    "+ $log \\mid FFT(X)^2 \\mid$\n",
    "\n",
    "This can then describe the frequency content in a local window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a DNN/RNN from which we can extract a transcription, trained from labeled pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Main issue_**: length(x) != length(y)\n",
    "\n",
    "We don't know how symbols in y map to frames of audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectionist Temporal Classification (CTC)\n",
    "\n",
    "1. RNN output neurons $c$ encode distro over symbols. length(c) == length(x)\n",
    "    + For phoneme-based model: $c \\in \\{AA,AE,AX,...,ER1,blank\\}$\n",
    "    \n",
    "    + For grapheme-based model: $c \\in \\{A,B,C,D,...,Z,blank,space\\}$\n",
    "\n",
    "2. Define a mapping $\\beta(c) \\rightarrow y$\n",
    "\n",
    "3. Max likelihood of $y*$ under this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Encoding_**\n",
    "\n",
    "Output neurons define distro over whole character seqs $c$ assuing independence, via the following formula:\n",
    "\n",
    "$ P(c\\mid x) \\equiv \\prod_{i=1}^{N} P(c_i\\mid x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Mapping_**\n",
    "\n",
    "Given a specific char seq $c$, squeeze out duplicates and blanks to yield a useful transcription.\n",
    "\n",
    "This mapping implies a distro over _possible transcriptions_ $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now understand the basics of the CTC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# This won't work outside the actual lib, hence the magic script call\n",
    "\"\"\"\n",
    "Define functions used to construct a multilayer GRU CTC model, and\n",
    "functions for training and testing it.\n",
    "\"\"\"\n",
    "\n",
    "import ctc\n",
    "import logging\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import (BatchNormalization, Convolution1D, Dense,\n",
    "                          Input, GRU, TimeDistributed)\n",
    "from keras.models import Model\n",
    "# from keras.optimizers import SGD\n",
    "import lasagne\n",
    "\n",
    "from utils import conv_output_length\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def compile_train_fn(model, learning_rate=2e-4):\n",
    "    \"\"\" Build the CTC training routine for speech models.\n",
    "    Args:\n",
    "        model: A keras model (built=True) instance\n",
    "    Returns:\n",
    "        train_fn (theano.function): Function that takes in acoustic inputs,\n",
    "            and updates the model. Returns network outputs and ctc cost\n",
    "    \"\"\"\n",
    "    logger.info(\"Building train_fn\")\n",
    "    acoustic_input = model.inputs[0]\n",
    "    network_output = model.outputs[0]\n",
    "    output_lens = K.placeholder(ndim=1, dtype='int32')\n",
    "    label = K.placeholder(ndim=1, dtype='int32')\n",
    "    label_lens = K.placeholder(ndim=1, dtype='int32')\n",
    "    network_output = network_output.dimshuffle((1, 0, 2))\n",
    "\n",
    "    ctc_cost = ctc.cpu_ctc_th(network_output, output_lens,\n",
    "                              label, label_lens).mean()\n",
    "    trainable_vars = model.trainable_weights\n",
    "    # optimizer = SGD(nesterov=True, lr=learning_rate, momentum=0.9,\n",
    "    #                 clipnorm=100)\n",
    "    # updates = optimizer.get_updates(trainable_vars, [], ctc_cost)\n",
    "    trainable_vars = model.trainable_weights\n",
    "    grads = K.gradients(ctc_cost, trainable_vars)\n",
    "    grads = lasagne.updates.total_norm_constraint(grads, 100)\n",
    "    updates = lasagne.updates.nesterov_momentum(grads, trainable_vars,\n",
    "                                                learning_rate, 0.99)\n",
    "    train_fn = K.function([acoustic_input, output_lens, label, label_lens,\n",
    "                           K.learning_phase()],\n",
    "                          [network_output, ctc_cost],\n",
    "                          updates=updates)\n",
    "    return train_fn\n",
    "\n",
    "\n",
    "def compile_test_fn(model):\n",
    "    \"\"\" Build a testing routine for speech models.\n",
    "    Args:\n",
    "        model: A keras model (built=True) instance\n",
    "    Returns:\n",
    "        val_fn (theano.function): Function that takes in acoustic inputs,\n",
    "            and calculates the loss. Returns network outputs and ctc cost\n",
    "    \"\"\"\n",
    "    logger.info(\"Building val_fn\")\n",
    "    acoustic_input = model.inputs[0]\n",
    "    network_output = model.outputs[0]\n",
    "    output_lens = K.placeholder(ndim=1, dtype='int32')\n",
    "    label = K.placeholder(ndim=1, dtype='int32')\n",
    "    label_lens = K.placeholder(ndim=1, dtype='int32')\n",
    "    network_output = network_output.dimshuffle((1, 0, 2))\n",
    "\n",
    "    ctc_cost = ctc.cpu_ctc_th(network_output, output_lens,\n",
    "                              label, label_lens).mean()\n",
    "    val_fn = K.function([acoustic_input, output_lens, label, label_lens,\n",
    "                        K.learning_phase()],\n",
    "                        [network_output, ctc_cost])\n",
    "    return val_fn\n",
    "\n",
    "\n",
    "def compile_output_fn(model):\n",
    "    \"\"\" Build a function that simply calculates the output of a model\n",
    "    Args:\n",
    "        model: A keras model (built=True) instance\n",
    "    Returns:\n",
    "        output_fn (theano.function): Function that takes in acoustic inputs,\n",
    "            and returns network outputs\n",
    "    \"\"\"\n",
    "    logger.info(\"Building val_fn\")\n",
    "    acoustic_input = model.inputs[0]\n",
    "    network_output = model.outputs[0]\n",
    "    network_output = network_output.dimshuffle((1, 0, 2))\n",
    "\n",
    "    output_fn = K.function([acoustic_input, K.learning_phase()],\n",
    "                           [network_output])\n",
    "    return output_fn\n",
    "\n",
    "\n",
    "def compile_gru_model(input_dim=161, output_dim=29, recur_layers=3, nodes=1024,\n",
    "                      conv_context=11, conv_border_mode='valid', conv_stride=2,\n",
    "                      initialization='glorot_uniform', batch_norm=True):\n",
    "    \"\"\" Build a recurrent network (CTC) for speech with GRU units \"\"\"\n",
    "    logger.info(\"Building gru model\")\n",
    "    # Main acoustic input\n",
    "    acoustic_input = Input(shape=(None, input_dim), name='acoustic_input')\n",
    "\n",
    "    # Setup the network\n",
    "    conv_1d = Convolution1D(nodes, conv_context, name='conv1d',\n",
    "                            border_mode=conv_border_mode,\n",
    "                            subsample_length=conv_stride, init=initialization,\n",
    "                            activation='relu')(acoustic_input)\n",
    "    if batch_norm:\n",
    "        output = BatchNormalization(name='bn_conv_1d', mode=2)(conv_1d)\n",
    "    else:\n",
    "        output = conv_1d\n",
    "\n",
    "    for r in range(recur_layers):\n",
    "        output = GRU(nodes, activation='relu',\n",
    "                     name='rnn_{}'.format(r + 1), init=initialization,\n",
    "                     return_sequences=True)(output)\n",
    "        if batch_norm:\n",
    "            bn_layer = BatchNormalization(name='bn_rnn_{}'.format(r + 1),\n",
    "                                          mode=2)\n",
    "            output = bn_layer(output)\n",
    "\n",
    "    # We don't softmax here because CTC does that\n",
    "    network_output = TimeDistributed(Dense(\n",
    "        output_dim, name='dense', activation='linear', init=initialization,\n",
    "    ))(output)\n",
    "    model = Model(input=acoustic_input, output=network_output)\n",
    "    model.conv_output_length = lambda x: conv_output_length(\n",
    "        x, conv_context, conv_border_mode, conv_stride)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding max likelihood of $\\theta$\n",
    "\n",
    "$\\theta * = argmax_\\theta \\sum_i log^P(y^{*(i)} \\mid x^{(i)})$\n",
    "\n",
    "which is\n",
    "\n",
    "$\\theta * = argmax_\\theta \\sum_i log \\sum_{c:\\beta (c) = y^{*(i)}} P (c \\mid x^{(i)})$\n",
    "\n",
    "[The CTC paper](http://www.cs.toronto.edu/~graves/icml_2006.pdf) provides a DP algorithm to compute the inner sum & its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libs efficiently do this for us these days, no need to write by hand:\n",
    "\n",
    "+ Warp CTC [`baidu-research/warp-ctc`](https://github.com/baidu-research/warp-ctc)\n",
    "+ Stanford CTC [`amaas/stanford-ctc`](https://github.com/amaas/stanford-ctc)\n",
    "+ Tensorflow `tf.nn.ctc_loss`\n",
    "\n",
    "These work by computing the following loss function and provide the $\\nabla$ w.r.t. _c_:\n",
    "\n",
    "$$L(\\theta) = log P(y^{*(i)} \\mid x^{(i)}) = CTC(c^{(i)}, y^{*(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/personal-notes/tricks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorter utterances result in simpler RNNs, and this helps reduce underflow/overflow and other network issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Decoding_**\n",
    "\n",
    "Approximate solution is so-called _max decoding_:\n",
    "\n",
    "$$\\beta (argmax_c P(c \\mid x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is \"often terrible\" but useful as a diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram roughly matches the code we looked at earlier:\n",
    "\n",
    "![](https://storage.googleapis.com/personal-notes/dlspeech_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Language models_** are a useful tool\n",
    "\n",
    "+ Basic strategy: use beam search to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-implementation of the decoding process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# we have a set of transcript prefixes A, candidates in a list T,\n",
    "# a language model function LM and an audio model function AM\n",
    "def decode(AM, LM, A, T):\n",
    "    for t in T:\n",
    "        for c in A:\n",
    "            add_blank()\n",
    "            update_probability(AM)\n",
    "            add_space()\n",
    "            udpdate_probability(LM)\n",
    "            add_char()\n",
    "            update_probability(AM)\n",
    "            A_prime = A\n",
    "            A_prime.append(c)\n",
    "    return k_most_probable(A_prime)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescoring with a neural LM can enhance N-gram trained from big corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application design is very important for a successful deep speech pipeline.\n",
    "\n",
    "We want to find data that matches our goals.\n",
    "\n",
    "|Styles of speech|Issues|Applications|\n",
    "|-|-|-|\n",
    "|Read|Disfluency/stuttering|Dictation|\n",
    "|Conversational|Noise|Meeting transcription|\n",
    "|Spontaneous|Mic quality/#channels|Call centers|\n",
    "|Command/control|Far field|Device control|\n",
    "||Reverb/echo|Mobile texting|\n",
    "||[Lombard effect](https://en.wikipedia.org/wiki/Lombard_effect)|Home/IoT/Cars|\n",
    "||Speaker accents||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive noise can help by synthesizing noisy environments.\n",
    "\n",
    "Engineer the data pipeline to be robust against noise, **_not the recognition pipeline!_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware of inefficiencies in OTS code, and make sure to pay special attention to minibatch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
